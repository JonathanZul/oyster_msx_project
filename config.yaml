# ===================================================================
#               MASTER CONFIGURATION FOR MSX PROJECT
# ===================================================================

# 1. Project Paths
# Define all the major input and output paths for the project.
# -------------------------------------------------------------------
paths:
  # Input data
  raw_wsis: "/Volumes/One Touch/MSX Project/data/exports/wsis"
  qupath_exports: "/Volumes/One Touch/MSX Project/data/raw/qupath_exports"

  # Oyster instance masks
  oyster_masks: "/Volumes/One Touch/MSX Project/data/interim/oyster_masks"

  # Processed data
  yolo_dataset: "/Volumes/One Touch/MSX Project/data/processed/yolo_dataset"

  # Model training outputs
  model_output_dir: "models/"

  # Inference and post-processing outputs
  inference_results: "/Volumes/One Touch/MSX Project/outputs/inference_results"
  qupath_imports: "/Volumes/One Touch/MSX Project/outputs/qupath_imports"

  # Logging directory
  logs: "logs/"

# --- Paths for the archived U-Net segmentation approach ---
  tensorboard_log_dir: "logs/tensorboard_logs/"
  segmentation_annotations: "/Volumes/One Touch/MSX Project/data/interim/segmentation_annotations"
  segmentation_dataset: "/Volumes/One Touch/MSX Project/data/processed/segmentation_dataset"
  segmentation_model_dir: "models/segmentation_model"

# 2. SAM-Based Oyster Segmentation
# Parameters controlling the `00_segment_with_sam.py` script.
# -------------------------------------------------------------------
sam_segmentation:
  # --- Prompt Generation (Classical CV) ---
  # Parameters for finding the oyster centroids.
  prompt_overview_downsample: 32.0
  gaussian_blur_kernel: [3, 3]
  adaptive_thresh_block_size: 11
  adaptive_thresh_c: 2
  morph_close_kernel: [5, 5]
  morph_close_iter: 2
  morph_open_kernel: [3, 3]
  morph_open_iter: 2

  # --- Watershed Segmentation ---
  watershed_erosion_iterations: 2
  num_oysters_to_detect: 2
  min_contour_area_percent: 0.10   # A contour must be at least 1% of the image.
  max_contour_area_percent: 0.45   # A contour cannot be more than 60% of the image.
  min_fragment_area: 200


  # --- SAM Inference ---
  # Parameters for running the original Segment Anything Model.
  sam_overview_downsample: 8.0
  pos_points_per_oyster: 1  # Number of positive points to use as prompts per oyster.

  # Official SAM Checkpoint URL and corresponding model type.
  # Using the most powerful ViT-Huge model.
  sam_checkpoint_url: "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
  sam_model_type: "vit_h"

  min_mask_area: 5000

  # --- Debugging ---
  save_debug_images: true
  debug_img_max_dim: 1024

evaluation:
  save_debug_visuals: True # Save debug images during evaluation

# 3. Dataset Creation Parameters
# Parameters controlling the `01_create_dataset.py` script.
# -------------------------------------------------------------------
dataset_creation:
  patch_size: 640 # The size of square patches (in pixels) to create.
  train_val_split: 0.8 # 80% for training, 20% for validation.
  append_mode: false # If true, new patches are added to the existing dataset instead of wiping it.
  classes:
    "MSX Plasmodia": 0
    "MSX Spores": 1
    "Waste": 2
    "Hemocyte": 3
    "Ignore": 4
    "Potential Confuser": 5
    "Stain": 6
    "?": 7

# 4. Training Parameters
# Parameters controlling the `02_train_yolo.py` script.
# -------------------------------------------------------------------
training:
  yolo_model: "yolo11n.pt" # The pre-trained model to start from.
  use_latest_model: false # If true, automatically find and use the most recent best.pt from models/
  epochs: 100
  batch_size: 16
  device: "mps" # Options: "cpu", "cuda:0", "mps" (for Apple Silicon)
  # NOTE: if running using mps use `PYTORCH_ENABLE_MPS_FALLBACK=1 python3 -m src.main_scripts.02_train_yolo` to prevent 'torchvision:nms' not implemented for MPS error
  img_size: 640 # Image size for training, should match patch_size.

# 5. Inference Parameters
# Parameters controlling the `03_run_inference.py` script.
# -------------------------------------------------------------------
inference:
  model_checkpoint: "models/yolov11m_msx_oyster_run7/weights/best.pt" # The path to your best trained model.
  patch_overlap: 0.2 # 20% overlap when creating patches for inference.
  conf_threshold: 0.25 # Confidence threshold for predictions.


# ===================================================================
#               CROSS-VALIDATION CONFIGURATION
# ===================================================================
cross_validation:
  # The segmentation methods to compare. Options: "watershed", "sam", "unet"
  methods_to_run: ["watershed", "sam", "unet"]

  # K-Fold settings
  n_splits: 5
  data_split_seed: 42 # Use a seed for reproducible folds

  # U-Net specific settings for training during cross-validation
  unet_epochs_per_fold: 25 # A smaller number of epochs for speed

  # Directory to store all temporary outputs for each fold
  temp_output_dir: "/Volumes/One Touch/MSX Project/outputs/cross_validation_temp/"



# -------------------------------------------------------------------
# ARCHIVED Classical computer vision parameters
#
# Controls the `archive/00_segment_oysters_classical.py` script.
# -------------------------------------------------------------------
oyster_segmentation:
  # Processing parameters
  processing_downsample: 32.0
  num_oysters_to_detect: 2

  # Image pre-processing parameters for tissue detection
  gaussian_blur_kernel: [3, 3]
  adaptive_thresh_block_size: 11
  adaptive_thresh_c: 2
  morph_close_kernel: [5, 5]
  morph_close_iter: 2
  morph_open_kernel: [3, 3]
  morph_open_iter: 2
  min_contour_area_percent: 0.01

  # Watrershed separation parameters
  use_watershed_separation: true
  watershed_erosion_iterations: 8

  # Robust Fragment Merging
  enable_fragment_merging: true
  min_fragment_area_percent: 0.001 # Fragments smaller than this % of the image will be discarded as noise

  # Debugging
  save_debug_images: true
  debug_img_max_dim: 1024

# -------------------------------------------------------------------
# ARCHIVED ML-based segmentation parameters
#
# Controls the `archive/unet_*` scripts.
# -------------------------------------------------------------------
ml_segmentation:
  # --- Data Creation (for 00a_create_dataset.py) ---
  overview_downsample: 32.0 # Downsample factor to extract WSI overviews
  image_size: [768, 1024] # Target size to resize all images/masks to [H, W]
  classes:
    "Oyster 1": 1                # NOTE: Class indices were 1-based for this script
    "Oyster 2": 2

  # --- Model Training (for unet_00b_train_model.py) ---
  device: "mps"                 # Device to use for training (options: "cpu", "cuda:0", "mps")
  encoder: "resnet34"           # Backbone for the U-Net
  encoder_weights: "imagenet"   # Use pre-trained weights
  epochs: 100                   # Total number of epochs to train the segmentation model
  batch_size: 4                 # Batch size for training
  learning_rate: 0.001          # Initial learning rate for training
  freeze_epochs: 30             # Number of epochs to train with the encoder frozen
  finetune_lr_factor: 0.01   # Factor to reduce LR by for the fine-tuning phase (0.001 -> 0.0001)
  selection_val_split: false     # Whether to use a selection split for training/validation
  data_split_seed: 42          # Random seed for splitting dataset into train/val sets
  train_val_split: 0.8          # Fraction of data to use for training (80% train, 20% val)
  early_stopping_patience: 20   # Early stopping patience for training

  # --- Inference (for unet_00_run_inference.py) ---
  model_checkpoint: "models/segmentation_model/best_model.pt" # Path to the trained segmentation model
  confidence_threshold: 0.5     # Probability threshold to create binary mask
  min_object_size: 1000         # Minimum pixel area to keep a predicted object after inference
  save_visualization: true      # Save visualizations of the segmentation results
