import argparse
import urllib.request
from pathlib import Path

import cv2
import numpy as np
import torch
from segment_anything import SamPredictor, sam_model_registry
from tqdm import tqdm

from src.utils.file_handling import load_config
from src.utils.logging_config import setup_logging, log_config
from src.utils.wsi_utils import extract_downsampled_overview, find_matching_wsi_path


# --- Helper Functions ---

def _save_debug_image(image_data: np.ndarray, filename_suffix: str, output_dir: Path, config: dict):
    """
    Saves a downsampled debug image if enabled in the config.

    Args:
        image_data (np.ndarray): The image data to save.
        filename_suffix (str): Suffix for the saved filename.
        output_dir (Path): Directory to save the debug image.
        config (dict): Configuration dictionary containing settings.

    Returns:
        None
    """
    if not config.get("save_debug_images", False):
        return

    debug_img_dir = output_dir / "debug_images"
    debug_img_dir.mkdir(exist_ok=True)
    save_path = str(debug_img_dir / f"{filename_suffix}.png")

    max_dim = config.get("debug_img_max_dim", 1024)
    h, w = image_data.shape[:2]

    # Resize image for smaller file size while maintaining aspect ratio.
    if max(h, w) > max_dim:
        scale = max_dim / max(h, w)
        new_w, new_h = int(w * scale), int(h * scale)
        resized_image = cv2.resize(image_data, (new_w, new_h), interpolation=cv2.INTER_AREA)
    else:
        resized_image = image_data

    # Save the image in the correct color format.
    if resized_image.ndim == 3:  # RGB
        cv2.imwrite(save_path, cv2.cvtColor(resized_image, cv2.COLOR_RGB2BGR))
    else:  # Grayscale or binary
        cv2.imwrite(save_path, resized_image)


def _visualize_selection(chosen_mask: np.ndarray, proxy_mask: np.ndarray, output_dir: Path, config: dict,
                         oyster_id: int):
    """
    Creates a visualization comparing the chosen SAM mask against the CV-generated proxy mask.
    - Green: True Positives (Overlap)
    - Red: False Negatives (In CV proxy but missed by SAM)
    - Yellow: False Positives (In SAM but not in CV proxy)

    Args:
        chosen_mask (np.ndarray): The mask selected by SAM.
        proxy_mask (np.ndarray): The ground truth mask generated by the CV pipeline.
        output_dir (Path): Directory to save the visualization.
        config (dict): Configuration dictionary containing settings.
        oyster_id (int): Identifier for the oyster being processed, used in the filename.

    Returns:
        None
    """
    h, w = chosen_mask.shape
    viz_image = np.zeros((h, w, 3), dtype=np.uint8)

    true_positives = np.logical_and(chosen_mask, proxy_mask)
    false_negatives = np.logical_and(proxy_mask, np.logical_not(chosen_mask))
    false_positives = np.logical_and(chosen_mask, np.logical_not(proxy_mask))

    viz_image[false_negatives] = [0, 0, 255]  # Red
    viz_image[false_positives] = [0, 255, 255]  # Green
    viz_image[true_positives] = [0, 255, 0]  # Yellow

    _save_debug_image(viz_image, f"06_selection_compare_oyster_{oyster_id}", output_dir, config)


def _visualize_repair_process(
        image: np.ndarray,
        initial_masks: list,
        repaired_masks: list,
        output_dir: Path,
        config: dict
):
    """
    Creates a side-by-side visualization to compare the segmentation masks
    before and after the fragment repair process.

    Args:
        image (np.ndarray): The original image on which masks were applied.
        initial_masks (list): List of initial SAM masks before repair.
        repaired_masks (list): List of repaired masks after the repair process.
        output_dir (Path): Directory to save the visualization.
        config (dict): Configuration dictionary containing settings.

    Returns:
        None
    """
    if not config.get("save_debug_images", False):
        return

    colors = [(255, 0, 0), (0, 0, 255)]  # Blue, Red

    # Create the "Before Repair" visualization
    viz_before = image.copy()
    cv2.putText(viz_before, "Before Repair", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 0, 0), 5)
    for i, mask in enumerate(initial_masks):
        if mask is not None and mask.any():
            color = colors[i % len(colors)]
            colored_overlay = np.zeros_like(viz_before)
            colored_overlay[mask > 0] = color
            viz_before = cv2.addWeighted(viz_before, 1.0, colored_overlay, 0.5, 0)
            contours, _ = cv2.findContours((mask * 255).astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            cv2.drawContours(viz_before, contours, -1, color, 2)

    # Create the "After Repair" visualization
    viz_after = image.copy()
    cv2.putText(viz_after, "After Repair", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 0, 0), 5)
    for i, mask in enumerate(repaired_masks):
        if mask is not None and mask.any():
            color = colors[i % len(colors)]
            colored_overlay = np.zeros_like(viz_after)
            # We need to clean the final mask for a fair visualization
            cleaned_mask_uint8 = clean_mask((mask * 255).astype(np.uint8), config.get("min_mask_area", 5000))
            colored_overlay[cleaned_mask_uint8 > 0] = color
            viz_after = cv2.addWeighted(viz_after, 1.0, colored_overlay, 0.5, 0)
            contours, _ = cv2.findContours(cleaned_mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            cv2.drawContours(viz_after, contours, -1, color, 2)

    # Combine and Save
    # Put the two images into a single image
    comparison_image = np.vstack([viz_before, viz_after])

    _save_debug_image(comparison_image, "06_repair_comparison", output_dir, config)


def clean_mask(mask: np.ndarray, min_size: int) -> np.ndarray:
    """
    Cleans a binary mask by keeping only the largest connected component above a min size.

    Args:
        mask (np.ndarray): The binary mask to clean.
        min_size (int): Minimum size of the component to keep.

    Returns:
        np.ndarray: The cleaned binary mask with only the largest component above min_size.
    """
    # Find connected components in the mask
    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(mask, connectivity=8)

    # If no labels found or only background, return an empty mask
    if num_labels <= 1:
        return np.zeros_like(mask)

    # Find the largest component
    largest_component_label = -1
    max_area = -1
    for i in range(1, num_labels):
        area = stats[i, cv2.CC_STAT_AREA]
        if area > max_area:
            max_area = area
            largest_component_label = i

    # If the largest component is smaller than min_size, return an empty mask
    cleaned_mask = np.zeros_like(mask)
    if max_area > min_size:
        cleaned_mask[labels == largest_component_label] = 255
    return cleaned_mask


def _visualize_prompts(
        image: np.ndarray,
        bbox: np.ndarray,
        pos_points: np.ndarray,
        neg_points: np.ndarray,
        filename_suffix: str,
        output_dir: Path,
        config: dict
):
    """
    Draws a single, complete prompt set (box, positive, and negative points) on an image.

    Args:
        image (np.ndarray): The original image on which to draw the prompt.
        bbox (np.ndarray): The bounding box for the target oyster in the format [x1, y1, x2, y2].
        pos_points (np.ndarray): Array of positive points as [x, y] coordinates.
        neg_points (np.ndarray): Array of negative points as [x, y] coordinates.
        filename_suffix (str): Suffix for the saved filename.
        output_dir (Path): Directory to save the visualization.
        config (dict): Configuration dictionary containing settings.

    Returns:
        None
    """
    viz_image = image.copy()  # Create a copy to avoid modifying the original image
    box_color = (0, 0, 255)  # Blue for the target bbox
    pos_color = (0, 255, 0)  # Green for positive points
    neg_color = (255, 0, 0)  # Red for negative points

    # Draw the bounding box for the target oyster
    x1, y1, x2, y2 = bbox
    cv2.rectangle(viz_image, (x1, y1), (x2, y2), box_color, 3)

    # Draw the positive points for the target oyster
    if pos_points is not None and len(pos_points) > 0:
        for point in pos_points:
            center = tuple(point.astype(int))
            cv2.circle(viz_image, center, 50, pos_color, -1)
            cv2.circle(viz_image, center, 50, (0, 0, 0), 2)

    # Draw the negative points
    if neg_points is not None and len(neg_points) > 0:
        for point in neg_points:
            center = tuple(point.astype(int))
            cv2.circle(viz_image, center, 50, neg_color, -1)
            cv2.circle(viz_image, center, 50, (0, 0, 0), 2)

    _save_debug_image(viz_image, filename_suffix, output_dir, config)


def generate_prompts_from_wsi(wsi_path: Path, config: dict, logger, debug_output_dir: Path) -> tuple[
    list, np.ndarray | None, np.ndarray | None]:
    """
    Generates rich prompts for SAM using a hierarchical CV pipeline. It first tries a simple
    contour-based approach, falls back to a targeted Watershed, and has a final geometric split fallback.

    Args:
        wsi_path (Path): Path to the Whole Slide Image file.
        config (dict): Configuration dictionary containing settings for the CV pipeline.
        logger: Logger instance for logging messages.
        debug_output_dir (Path): Directory to save debug images if enabled in config.

    Returns:
        tuple: A tuple containing:
            - List of prompts, each with bounding box, centvlm
            roid, and contour points.
            - Overview image of the WSI.
            - Opened mask image after morphological operations.
    """
    logger.info("Generating prompts using hybrid CV pipeline...")
    overview_img = extract_downsampled_overview(wsi_path, config["prompt_overview_downsample"], logger)
    if overview_img is None:  # Failed to extract overview
        return [], None, None
    _save_debug_image(overview_img, "01_prompt_overview", debug_output_dir, config)

    # --- Step 1: Get a clean binary mask of all tissue ---
    gray = cv2.cvtColor(overview_img, cv2.COLOR_RGB2GRAY)
    blurred = cv2.GaussianBlur(gray, tuple(config["gaussian_blur_kernel"]), 0)
    binary = cv2.adaptiveThreshold(
        blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV,
        config["adaptive_thresh_block_size"], config["adaptive_thresh_c"]
    )
    k_close = np.ones(tuple(config["morph_close_kernel"]), np.uint8)
    closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, k_close, iterations=config["morph_close_iter"])
    k_open = np.ones(tuple(config["morph_open_kernel"]), np.uint8)
    opened_mask = cv2.morphologyEx(closed, cv2.MORPH_OPEN, k_open, iterations=config["morph_open_iter"])
    _save_debug_image(opened_mask, "02_morph_opened", debug_output_dir, config)

    # --- Step 2: Attempt the simple contour filtering approach first ---
    contours, _ = cv2.findContours(opened_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    image_area = overview_img.shape[0] * overview_img.shape[1]
    min_area = image_area * config["min_contour_area_percent"]
    max_area = image_area * config["max_contour_area_percent"]

    valid_contours = [c for c in contours if min_area < cv2.contourArea(c) < max_area]
    logger.info(f"Found {len(contours)} initial contours, {len(valid_contours)} remain after size filtering.")

    oyster_contours = []
    if len(valid_contours) >= config["num_oysters_to_detect"]:
        logger.info("Sufficient well-separated contours found. Using simple approach.")
        sorted_contours = sorted(valid_contours, key=cv2.contourArea, reverse=True)
        oyster_contours = sorted_contours[:config["num_oysters_to_detect"]]
    else:
        # --- Step 3: "Rescue" Logic using Targeted Watershed ---
        logger.warning("Simple approach failed. Activating Watershed rescue for touching oysters.")
        # Find the single, largest contour, regardless of its size.
        merged_contour = max(contours, key=cv2.contourArea, default=None)

        if merged_contour is not None:
            targeted_mask = np.zeros_like(opened_mask)
            cv2.drawContours(targeted_mask, [merged_contour], -1, 255, -1)
            _save_debug_image(targeted_mask, "03a_watershed_target_mask", debug_output_dir, config)

            # Perform Watershed on this targeted region.
            dist_transform = cv2.distanceTransform(targeted_mask, cv2.DIST_L2, 5)
            _, sure_fg = cv2.threshold(dist_transform, config["watershed_erosion_iterations"] * dist_transform.mean(),
                                       255, 0)
            sure_fg = np.uint8(sure_fg)
            num_labels, markers = cv2.connectedComponents(sure_fg)
            _save_debug_image((markers * (255 // num_labels)).astype(np.uint8) if num_labels > 0 else markers,
                              "03b_watershed_seeds", debug_output_dir, config)

            if num_labels > 1:  # If we have multiple labels, we can proceed with Watershed.
                markers = markers + 1
                unknown = cv2.subtract(cv2.dilate(targeted_mask, k_open, iterations=3), sure_fg)
                markers[unknown == 255] = 0
                markers = cv2.watershed(cv2.cvtColor(opened_mask, cv2.COLOR_GRAY2BGR), markers)
                _save_debug_image((markers * (255 // (num_labels + 1))).astype(np.uint8), "03c_watershed_result_labels",
                                  debug_output_dir, config)

                # Extract the two largest resulting objects from the Watershed.
                found_contours = []
                for i in range(1, num_labels + 1):
                    watershed_mask = (markers == i + 1).astype(np.uint8) * 255
                    if np.sum(watershed_mask) > min_area:
                        sub_contours, _ = cv2.findContours(watershed_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                        found_contours.extend(sub_contours)

                # Filter the found contours again based on size.
                if len(found_contours) >= config["num_oysters_to_detect"]:
                    oyster_contours = sorted(found_contours, key=cv2.contourArea, reverse=True)[
                                      :config["num_oysters_to_detect"]]
                    logger.info("Watershed rescue successful. Found separated contours.")

    # --- Step 4: Final Fallback - Geometric Split ---
    # If we still don't have enough contours, we will split the largest contour geometrically.
    # This is a last resort to ensure we always return at least two prompts.
    if not oyster_contours and merged_contour is not None:
        logger.warning("Watershed rescue failed. Activating final fallback: geometric split.")
        # Create a mask of the single largest contour
        fallback_mask = np.zeros_like(opened_mask)
        cv2.drawContours(fallback_mask, [merged_contour], -1, 255, -1)

        # Determine whether to split vertically or horizontally based on aspect ratio
        x, y, w, h = cv2.boundingRect(merged_contour)
        split_vertically = w > h

        mask1 = fallback_mask.copy()
        mask2 = fallback_mask.copy()
        if split_vertically:
            # Split down the middle vertically
            mid_x = x + w // 2
            mask1[:, mid_x:] = 0
            mask2[:, :mid_x] = 0
        else:
            # Split across the middle horizontally
            mid_y = y + h // 2
            mask1[mid_y:, :] = 0
            mask2[:mid_y, :] = 0

        # Find the contours of the two new halves
        contours1, _ = cv2.findContours(mask1, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        contours2, _ = cv2.findContours(mask2, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if contours1 and contours2:
            oyster_contours = [max(contours1, key=cv2.contourArea), max(contours2, key=cv2.contourArea)]
            logger.info("Geometric split fallback successful.")

    if not oyster_contours:  # If we still have no contours, log an error and return empty.
        logger.error("All prompt generation methods failed for this slide.")
        return [], None, None

    # --- Step 5: Generate Prompts from the final selected contours ---
    final_prompts = []
    for contour in oyster_contours:
        # Calculate the bounding box and centroid for the contour
        x, y, w, h = cv2.boundingRect(contour)
        bbox = np.array([x, y, x + w, h + y])
        moments = cv2.moments(contour)
        cx, cy = (0, 0)
        if moments["m00"] > 0:  # Avoid division by zero
            cx = int(moments["m10"] / moments["m00"])
            cy = int(moments["m01"] / moments["m00"])
        centroid = np.array([[cx, cy]])

        num_points = config["pos_points_per_oyster"]
        contour_points = contour.squeeze()
        indices = np.linspace(0, len(contour_points) - 1, num_points, dtype=int)
        sampled_points = contour_points[indices]

        final_prompts.append({
            "bbox": bbox,
            "centroid": centroid,
            "contour_points": sampled_points
        })

    logger.info(f"Generated prompts for {len(final_prompts)} oysters.")
    return final_prompts, overview_img, opened_mask


def initialize_sam_predictor(config: dict, device, logger) -> SamPredictor | None:
    """
    Downloads the official SAM checkpoint, initializes the model, and returns a predictor.

    Args:
        config (dict): Configuration dictionary containing model type and checkpoint URL.
        device: The device to run the model on (CPU or GPU).
        logger: Logger instance for logging messages.

    Returns:
        SamPredictor | None: The initialized SAM predictor or None if initialization fails.
    """
    logger.info("Initializing Segment Anything Model (SAM) predictor...")
    model_type = config["sam_model_type"]
    checkpoint_url = config["sam_checkpoint_url"]

    checkpoint_dir = Path("models/sam_checkpoints")
    checkpoint_dir.mkdir(parents=True, exist_ok=True)
    checkpoint_path = checkpoint_dir / Path(checkpoint_url).name

    if not checkpoint_path.exists():
        logger.info(f"Downloading SAM checkpoint from {checkpoint_url}...")
        try:
            urllib.request.urlretrieve(checkpoint_url, checkpoint_path)
            logger.info(f"Checkpoint downloaded successfully to: {checkpoint_path}")
        except Exception as e:
            logger.critical(f"Failed to download SAM checkpoint: {e}", exc_info=True)
            return None
    else:
        logger.info(f"Using cached SAM checkpoint: {checkpoint_path}")

    try:
        model = sam_model_registry[model_type](checkpoint=str(checkpoint_path))
        model.to(device=device)
        return SamPredictor(model)
    except Exception as e:
        logger.critical(f"Failed to initialize SAM model from checkpoint: {e}", exc_info=True)
        return None


def repair_and_merge_fragments(
        initial_masks_high_res: list,
        proxy_mask_low_res: np.ndarray,
        config: dict,
        logger,
        output_dir: Path
) -> list[np.ndarray]:
    """
    Finds tissue fragments missed by the initial SAM segmentation and merges them
    into the topologically closest parent mask. This version performs the expensive
    distance calculations in low-resolution space for a significant speed-up.

    Args:
        initial_masks_high_res (list): List of initial SAM masks in high resolution.
        proxy_mask_low_res (np.ndarray): The low-resolution proxy mask generated by the CV pipeline.
        config (dict): Configuration dictionary containing settings for the repair process.
        logger: Logger instance for logging messages.
        output_dir (Path): Directory to save debug images and results.

    Returns:
        list[np.ndarray]: List of repaired masks in high resolution, with orphan fragments merged into the closest parent.
    """
    logger.info("Starting optimized mask repair process to reclaim missed fragments...")

    if not initial_masks_high_res or all(m is None for m in initial_masks_high_res):
        logger.warning("No initial masks to repair. Returning as is.")
        return initial_masks_high_res

    # --- Step 1: Work in Low-Resolution Space ---
    low_res_shape = (proxy_mask_low_res.shape[1], proxy_mask_low_res.shape[0])  # (W, H)

    # Downscale the initial SAM masks to match the proxy mask's resolution.
    initial_masks_low_res = [
        cv2.resize((m * 255).astype(np.uint8), low_res_shape, interpolation=cv2.INTER_NEAREST)
        if m is not None else np.zeros_like(proxy_mask_low_res)
        for m in initial_masks_high_res
    ]

    # Find "orphan" tissue by subtracting segmented tissue from the total tissue proxy.
    # All masks are now low-resolution and uint8.
    segmented_tissue_low_res = np.zeros_like(proxy_mask_low_res)
    for mask in initial_masks_low_res:
        segmented_tissue_low_res = cv2.bitwise_or(segmented_tissue_low_res, mask)

    orphan_mask_low_res = cv2.subtract(proxy_mask_low_res, segmented_tissue_low_res)
    _save_debug_image(orphan_mask_low_res, "06a_orphan_fragments_low_res", output_dir, config)

    # --- Step 2: Assign Orphans in Low-Resolution Space ---
    min_area = config.get("min_fragment_area", 100)
    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(orphan_mask_low_res, connectivity=8)

    if num_labels <= 1:
        logger.info("No orphan fragments found to repair.")
        return initial_masks_high_res

    # Create a "gravity map" for each of the initial low-res SAM masks. THIS IS THE FAST PART.
    dist_maps = [cv2.distanceTransform(mask, cv2.DIST_L2, 5) for mask in initial_masks_low_res]

    # This dictionary will store which parent each orphan label belongs to.
    assignment_map = {}
    for i in range(1, num_labels):
        if stats[i, cv2.CC_STAT_AREA] < min_area: continue

        cx, cy = centroids[i].astype(int)
        # Find which initial SAM mask this fragment's centroid is topologically closest to.
        distances = [dist_map[cy, cx] for dist_map in dist_maps]
        closest_parent_idx = np.argmin(distances)
        assignment_map[i] = closest_parent_idx

    # --- Step 3: Merge Fragments in High-Resolution Space ---
    # We make the final merge in high-res to preserve detail.
    repaired_masks_high_res = [
        m.copy() if m is not None else np.zeros((config['image_size'][0], config['image_size'][1]), dtype=bool) for m in
        initial_masks_high_res]

    # To find the high-res orphans, we need a high-res total tissue mask.
    high_res_shape = (repaired_masks_high_res[0].shape[1], repaired_masks_high_res[0].shape[0])
    total_tissue_mask_high_res = cv2.resize(proxy_mask_low_res, high_res_shape, interpolation=cv2.INTER_NEAREST)

    segmented_high_res = np.zeros_like(repaired_masks_high_res[0], dtype=bool)
    for mask in initial_masks_high_res:
        if mask is not None:
            segmented_high_res = np.logical_or(segmented_high_res, mask)

    orphan_mask_high_res = np.logical_and(total_tissue_mask_high_res, np.logical_not(segmented_high_res))
    num_labels_high, labels_high, _, _ = cv2.connectedComponentsWithStats((orphan_mask_high_res * 255).astype(np.uint8))

    num_reclaimed = 0
    for i in range(1, num_labels_high):
        fragment_mask_high_res = (labels_high == i)
        # Find the low-res label corresponding to this high-res fragment
        # by checking the label at the fragment's center.
        frag_moments = cv2.moments((fragment_mask_high_res * 255).astype(np.uint8))
        if frag_moments["m00"] == 0: continue
        cx_high = int(frag_moments["m10"] / frag_moments["m00"])
        cy_high = int(frag_moments["m01"] / frag_moments["m00"])
        cx_low = int(cx_high * (low_res_shape[0] / high_res_shape[0]))
        cy_low = int(cy_high * (low_res_shape[1] / high_res_shape[1]))

        low_res_label = labels[cy_low, cx_low]
        if low_res_label in assignment_map:
            parent_idx = assignment_map[low_res_label]
            repaired_masks_high_res[parent_idx] = np.logical_or(repaired_masks_high_res[parent_idx],
                                                                fragment_mask_high_res)
            num_reclaimed += 1

    logger.info(f"Reclaimed and merged {num_reclaimed} orphan fragments.")
    return repaired_masks_high_res


def select_best_mask(
        masks: np.ndarray,
        scores: np.ndarray,
        proxy_gt_mask: np.ndarray,
        logger) -> tuple[np.ndarray, int]:
    """
    Selects the best mask from SAM's multimask output by comparing each candidate
    to a proxy ground truth mask using the Dice score. Returns the best mask and its index.

    Args:
        masks (np.ndarray): Array of candidate masks from SAM.
        scores (np.ndarray): Corresponding SAM scores for each mask.
        proxy_gt_mask (np.ndarray): The ground truth mask generated by the CV pipeline.
        logger: Logger instance for logging messages.

    Returns:
        tuple: A tuple containing:
            - The best mask selected based on the Dice score.
            - The index of the best mask in the original masks array.
    """
    best_mask = None
    best_dice_score = -1
    best_mask_idx = -1

    for i, (mask, score) in enumerate(zip(masks, scores)):
        if np.sum(mask) == 0: continue

        # Both masks are now in the same high-resolution space, so Dice is meaningful.
        intersection = np.sum(np.logical_and(mask, proxy_gt_mask))
        pred_sum = np.sum(mask)
        gt_sum = np.sum(proxy_gt_mask)
        dice_score = (2.0 * intersection) / (pred_sum + gt_sum + 1e-6)

        logger.debug(f"Candidate mask {i}: SAM score={score:.3f}, Dice score vs. CV proxy={dice_score:.3f}")

        if dice_score > best_dice_score:
            best_dice_score = dice_score
            best_mask = mask
            best_mask_idx = i

    # Confirmation logging.
    if best_mask_idx != -1:
        logger.info(f"Selected candidate mask {best_mask_idx} with the best Dice score of {best_dice_score:.4f}.")
    else:
        # Fallback to the highest SAM score if all Dice scores are zero.
        best_mask_idx = np.argmax(scores)
        best_mask = masks[best_mask_idx]
        logger.warning(f"All Dice scores were zero. Falling back to SAM's best score (mask {best_mask_idx}).")

    return best_mask, best_mask_idx


def process_single_wsi(wsi_path: Path, predictor: SamPredictor, config: dict, logger, base_output_dir: Path):
    """
    Processes a single WSI file to generate SAM segmentation masks for oysters.

    Steps:
        1. Generate prompts from the WSI using a CV pipeline.
        2. Extract a downsampled overview image for SAM.
        3. Scale the proxy mask to high-resolution coordinates.
        4. Set the overview image in the SAM predictor.
        5. Run iterative inference for each oyster prompt.
        6. Repair and merge fragments missed by SAM.
        7. Save and visualize the final masks.

    Args:
        wsi_path (Path): Path to the Whole Slide Image file.
        predictor (SamPredictor): Initialized SAM predictor instance.
        config (dict): Configuration dictionary containing settings for the segmentation.
        logger: Logger instance for logging messages.

    Returns:
        None
    """
    output_dir = base_output_dir / wsi_path.stem.replace(".ome", "")
    output_dir.mkdir(parents=True, exist_ok=True)

    # generate_prompts now returns the low-res proxy mask as well.
    oyster_prompts, overview_low_res, proxy_mask_low_res = generate_prompts_from_wsi(wsi_path,
                                                                                     config["sam_segmentation"], logger,
                                                                                     output_dir)
    if len(oyster_prompts) < 2:
        logger.error(f"Failed to generate prompts for 2 oysters in {wsi_path.name}. Skipping.")
        return

    overview_high_res = extract_downsampled_overview(wsi_path, config["sam_segmentation"]["sam_overview_downsample"],
                                                     logger)
    if overview_high_res is None: return

    low_res_h, low_res_w = overview_low_res.shape[:2]
    high_res_h, high_res_w = overview_high_res.shape[:2]
    scale_w, scale_h = high_res_w / low_res_w, high_res_h / low_res_h

    # Scale the proxy mask to the high-res coordinate space *before* using it for selection.
    proxy_mask_high_res = cv2.resize(
        proxy_mask_low_res, (high_res_w, high_res_h), interpolation=cv2.INTER_NEAREST
    ).astype(bool)

    # Scale the prompts to the high-res coordinate space.
    for prompts in oyster_prompts:
        prompts["bbox"] = (prompts["bbox"].astype(np.float32) * np.array([scale_w, scale_h, scale_w, scale_h])).astype(
            int)
        prompts["centroid"] = (prompts["centroid"].astype(np.float32) * np.array([scale_w, scale_h])).astype(int)
        prompts["contour_points"] = (
                prompts["contour_points"].astype(np.float32) * np.array([scale_w, scale_h])).astype(int)

    logger.info("Setting image in SAM predictor and running iterative inference...")
    predictor.set_image(overview_high_res)

    initial_masks = []
    for i in range(len(oyster_prompts)):
        target_prompts = oyster_prompts[i]
        other_prompts = oyster_prompts[1 - i]

        # --- Assemble the rich prompt package for SAM. ---
        box = target_prompts["bbox"]
        pos_points = target_prompts["centroid"]  # The single, reliable cluster center

        # 1. Take the bounding box of the OTHER oyster.
        other_box = other_prompts["bbox"]
        ox1, oy1, ox2, oy2 = other_box
        ow, oh = ox2 - ox1, oy2 - oy1

        # 2. Shrink this box inwards by a margin (e.g., 20% on each side).
        shrink_factor = 0.20
        shrunken_center_x = ox1 + ow // 2
        shrunken_center_y = oy1 + oh // 2

        # 3. The center of this shrunken box is a guaranteed-to-be-internal negative point.
        surefire_neg_point = np.array([[shrunken_center_x, shrunken_center_y]])

        # 4. Generate negative points from the bounding box of the OTHER oyster.
        x1, y1, x2, y2 = box
        neg_points_bbox = np.array([[x1, y1], [x2, y1], [x1, y2], [x2, y2]])
        neg_points = np.vstack([surefire_neg_point, neg_points_bbox])
        point_coords = np.vstack([pos_points, neg_points])
        point_labels = np.array([1] * len(pos_points) + [0] * len(neg_points))

        # Visualize the prompts for this oyster.
        _visualize_prompts(
            image=overview_high_res, bbox=box, pos_points=pos_points, neg_points=neg_points,
            filename_suffix=f"04_prompts_for_oyster_{i + 1}", output_dir=output_dir,
            config=config["sam_segmentation"]
        )

        masks, scores, _ = predictor.predict(
            point_coords=point_coords, point_labels=point_labels,
            box=box, multimask_output=True,
        )

        # Select the best mask based on the Dice score against the proxy mask.
        best_mask, best_mask_idx = select_best_mask(masks, scores, proxy_mask_high_res, logger)
        initial_masks.append(best_mask)

        # Visualize the selection process for this oyster.
        _visualize_selection(best_mask, proxy_mask_high_res, output_dir, config["sam_segmentation"], oyster_id=i + 1)

    # --- Repair and merge fragments missed by SAM ---
    final_masks = repair_and_merge_fragments(initial_masks, proxy_mask_low_res, config["sam_segmentation"], logger, output_dir)

    _visualize_repair_process(image=overview_high_res, initial_masks=initial_masks, repaired_masks=final_masks,
                              output_dir=output_dir, config=config["sam_segmentation"])

    # --- Post-process, save, and visualize the final selected masks ---
    viz_image = overview_high_res.copy()
    colors = [(255, 0, 0), (0, 0, 255)]  # Blue, Red
    for i, mask in enumerate(final_masks):
        if mask is None: continue
        mask_uint8 = mask.astype(np.uint8) * 255
        cleaned_mask = clean_mask(mask_uint8, config["sam_segmentation"]["min_mask_area"])
        if np.sum(cleaned_mask) == 0: continue

        mask_filename = output_dir / f"oyster_{i + 1}_mask.png"
        cv2.imwrite(str(mask_filename), cleaned_mask)
        logger.info(f"Saved final mask to {mask_filename}")

        color = colors[i % len(colors)]
        colored_overlay = np.zeros_like(viz_image)
        colored_overlay[cleaned_mask > 0] = color
        viz_image = cv2.addWeighted(viz_image, 1.0, colored_overlay, 0.5, 0)
        contours, _ = cv2.findContours(cleaned_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        cv2.drawContours(viz_image, contours, -1, color, 2)

    _save_debug_image(viz_image, "05_final_sam_segmentation", output_dir, config["sam_segmentation"])


def run_sam_pipeline(config: dict, logger, file_stems: list[str] | None = None, output_override_dir: Path | None = None):
    """
    Main execution logic for the SAM-based segmentation pipeline.

    Args:
        config (dict): The full project configuration.
        logger: The logger instance.
        file_stems (list[str] | None): An optional list of slide stems to process.
                                      If None, all slides in the raw_wsis dir will be processed.
    """
    logger.info("--- Running Hybrid SAM Segmentation Pipeline ---")

    base_output_dir = output_override_dir if output_override_dir is not None else Path(config["paths"]["oyster_masks"])

    # --- Device and Model Setup ---
    device_str = config["training"].get("device", "cpu")
    if device_str == "cuda" and not torch.cuda.is_available():
        logger.warning("CUDA not available. Falling back to CPU.")
        device_str = "cpu"
    elif device_str == "mps" and not torch.backends.mps.is_available():
        logger.warning("MPS not available. Falling back to CPU.")
        device_str = "cpu"
    device = torch.device(device_str)

    predictor = initialize_sam_predictor(config["sam_segmentation"], device, logger)
    if predictor is None:
        logger.critical("Could not initialize SAM predictor. Aborting.")
        return

    # --- Find WSIs and Run Inference ---
    wsi_dir = Path(config["paths"]["raw_wsis"])
    if file_stems:
        wsi_paths = []
        for stem in file_stems:
            annot_placeholder = wsi_dir / f"{stem}.geojson"
            wsi_path = find_matching_wsi_path(annot_placeholder, wsi_dir)
            if wsi_path:
                wsi_paths.append(wsi_path)
        logger.info(f"Processing a specific list of {len(wsi_paths)} slides.")
    else:
        logger.info("Processing all slides found in the raw WSI directory.")
        wsi_paths = [p for p in wsi_dir.iterdir() if
                     p.suffix.lower() in [".tif", ".tiff", ".vsi"] and not p.name.startswith('.')]

    if not wsi_paths:
        logger.error(f"No WSI files found to process.");
        return

    for wsi_path in tqdm(wsi_paths, desc="Segmenting WSIs with SAM"):
        process_single_wsi(wsi_path, predictor, config, logger, base_output_dir=base_output_dir)

    logger.info("--- Hybrid SAM Segmentation Finished ---")

def main():
    """
    Main function for the SAM-based segmentation pipeline.

    Parses command-line arguments, loads the configuration, initializes logging,
    sets up the SAM predictor, and processes all WSIs in the specified directory.
    """
    parser = argparse.ArgumentParser(description="Stage 00: Segment Oysters with SAM")
    parser.add_argument("-c", "--config", type=str, default="config.yaml", help="Path to the config file.")
    args = parser.parse_args()

    config = load_config(args.config)
    if not config: return

    logger = setup_logging(Path(config["paths"]["logs"]), "00_segment_with_sam")
    log_config(config, logger)
    logger.info("--- Starting Hybrid SAM Segmentation Pipeline (Script 00) ---")

    device_str = config["training"].get("device", "cpu")
    if device_str == "cuda" and not torch.cuda.is_available():
        logger.warning("CUDA not available. Falling back to CPU.")
        device_str = "cpu"
    elif device_str == "mps" and not torch.backends.mps.is_available():
        logger.warning("MPS not available. Falling back to CPU.")
        device_str = "cpu"
    device = torch.device(device_str)

    predictor = initialize_sam_predictor(config["sam_segmentation"], device, logger)
    if predictor is None:
        logger.critical("Could not initialize SAM predictor. Aborting.")
        return

    wsi_dir = Path(config["paths"]["raw_wsis"])
    wsi_paths = [p for p in wsi_dir.iterdir() if
                 p.suffix.lower() in [".tif", ".tiff", ".vsi"] and not p.name.startswith('.')]

    if not wsi_paths:
        logger.error(f"No WSI files found in '{wsi_dir}'. Exiting.")
        return

    logger.info(f"Found {len(wsi_paths)} WSIs to process.")
    for wsi_path in tqdm(wsi_paths, desc="Segmenting WSIs"):
        process_single_wsi(wsi_path, predictor, config, logger)

    logger.info("--- Hybrid SAM Segmentation Finished ---")


if __name__ == "__main__":
    """This block allows the script to be run standalone as before."""
    parser = argparse.ArgumentParser(description="Stage 00: Segment Oysters with SAM")
    parser.add_argument("-c", "--config", type=str, default="config.yaml", help="Path to the config file.")
    args = parser.parse_args()

    config = load_config(args.config)
    if config:
        logger = setup_logging(Path(config["paths"]["logs"]), "00_segment_with_sam")
        log_config(config, logger)
        run_sam_pipeline(config, logger)
